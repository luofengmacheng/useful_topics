## 2016年工作总结

> 前言：这是一篇工作总结，主要是工作过程中积累的一些经验和问题。

### 1 配置和日志

开始写程序时，就需要规划好整个程序的目录结构，将配置文件、程序、脚本、临时文件等都放在对应的目录中，例如：

```
bin 二进制程序
conf 配置文件
lib 调用的外部包
logs 日志文件
shell 启停或者日志清理脚本
template 使用的相关模版文件
tmp 临时文件
```

规划好目录结构后，就需要规划配置和日志。

程序启动后，首先要做的就是载入配置，配置包括日志的配置、数据库的配置、调用的外部接口配置、相关程序的路径配置等。首先，配置文件放在conf目录或者etc目录，其次，`不要将所有的配置都放在一个文件中，尽量将不同类别的配置放在不同的配置文件`，正如前面所提到的，可以将配置文件分为全局配置文件和局部配置文件，全局配置文件负责整个程序的配置，例如库文件所在的目录、接口的配置等，而局部配置文件负责某类配置，例如DB的配置、redis的配置等。

这里的原则就是：

* 程序中使用的任何路径或者配置值都不要使用常量，都要从配置文件中获取
* 不要将所有的配置都放在一个文件中，尽量将不同类别的配置放在不同的配置文件

日志的重要性不言而喻，它在调试、测试、定位问题过程中起着十分关键的作用，同时，它也是比较难以处理的一个部分。

不同的语言都提供了日志操作的库，但是使用日志模块的关键问题在于：

* 如何记录日志
* 记录什么内容

如何记录日志并不是如何使用日志模块，而是如何命名日志文件、如何组织日志文件。关于如何命名日志文件，常见的是按照天进行滚动，例如，日志文件格式为log.[YYYYMMDD]，在日志的配置中设置保留一个星期或者一个月的日志；关于如何组织日志文件，有的将所有的日志都打印到一个日志文件中，而有的将每个模块的日志分别打印到日志文件中，两种方式各有优劣，但是，这里比较建议使用将各个模块的日志分别打印到多个日志文件中，因为，在使用过程中发现，当产生的日志较多时，分别打印对于定位问题、查看系统的整体执行状况更加方便。

记录什么内容，这里涉及到两个问题：日志中要记录哪些部分、何时应该打印日志。关于要记录哪些部分，比较常见的有：时间、文件名、函数名、代码行、操作人等；关于何时应该打印日志，这个问题需要依据系统而定，个人也没有这方面的经验。

最后一个问题是关于数据库的日志。数据库的日志作为一个比较特殊的部分，对于数据的紧急恢复、问题查找很重要，而且某些系统的日志可能大部分都是数据库的操作日志，这时候可以将数据库的日志给独立出来，将除了查询以外的SQL语句都打印到单独的日志，这样便于问题的追踪和数据的恢复。

### 2 提高异步任务的执行效率

我们的系统用到了一个服务，使用该服务可以上传文件到服务器、在服务器执行脚本、从服务器下载文件等，因此，这里使用的模式是：将文件和发布脚本上传到目标机器，然后在目标机器执行发布脚本，并生成发布日志，最后对发布日志压缩，并下载到本地服务器。其中，该服务提供的每个操作都是异步的，也就是说，向该服务提交一个任务，如果任务提交成功会返回一个任务ID，后续可以通过该任务ID查询任务执行情况。将上面的操作进行封装成一个任务，那么，一次发布操作就是上面的一个任务。

为了提高发布的效率，可以同时执行多个任务，一种想法是多线程，同时开启多个线程，每个线程执行一个任务，但是，该服务不支持这种方式，它会并发请求数量。而且，即使开启多个线程，每个线程大部分的时间也是在定期查询任务的结果，而没有执行任何操作。针对这种情况，可以考虑使用IO多路复用的方式。

这里分享在网络上看到的关于IO模型的段子：

* 同步：一个人钓鱼，然后等着鱼上钩了再拉杆；
* 异步：鱼竿有个功能，能够显示是否由于上钩，因此，可以先去干别的事，过一段时间再来看是否有鱼上钩，如果有就拉杆；
* 多路复用：放多个鱼竿，一旦哪个鱼竿显示有鱼上钩就拉起对应的鱼竿

于是，采用多路复用的处理方式是：

创建一个任务池，并设置任务池的大小(即鱼竿的数量)，其中保存了每个任务所处的阶段，以及该任务的元数据信息(可以用于识别该任务)。将一个发布任务分解成多个阶段，每个阶段的操作很简单：根据当前所处的阶段完成不同的事情。例如，在任务刚开始阶段，它应该提交上传文件的任务，提交完成后则跳转到下一个阶段；在下一个阶段，就是查询上传任务是否完成，如果没有完成，继续处理，如果完成了，则跳转到再下一个阶段。

主程序的逻辑是：

* 查看是否有要处理的任务(这些任务可能来自数据库，也可能来自消息队列)
* 获取要处理的任务，将它放到任务池中，并初始化它所处的阶段值
* 根据它所处的阶段完成不同的事情

### 系统重构过程中遇到的问题

#### 1 文件编码

* 文件写入时编码的检测

用程序对一个文件进行写入时，必须保证不对文件本身的编码做任何改变。如果改变了编码，那么其它程序在进行处理时会由于编码问题导致处理失败(例如python2)。那么，如何用程序来获取文件编码？或者文件本身包含编码信息吗？

当文件使用了BOM(Byte Order Mark)时，会在文件开头写入个字符用于标记文件的编码，但是，这种方案的缺点是，并不是所有的编辑器都能够识别该标记，不同的编辑器对于该标记的处理方式也可能不同，而且，有些编程语言可能会将这些不可见的字符作为文本的一部分(例如php)。

因此，这里考虑的是不带BOM的文件。由于没有携带编码格式，程序只能采用跟vi编辑器类似的方式:"猜"。大家知道，在vi编辑器中可以执行`set fileencoding`查看文件的编码，但事实上，该编码并不一定是文件的编码。vi会通过自己的配置对文件的编码进行猜测，当猜中哪个，就是哪个，因此，如果某个文件的二进制出现在两种编码中，在不同的vi配置，有可能返回的结果是不一样，而且，显示的内容也可能是不一样的。但是，在没有给定任何编码信息的基础上，也只能通过这种方式进行探测，值得庆幸的是，当给编码限定一个范围，那么，进行探测时，多半还是可以得到正确结果的。例如，在我们的场景中，只会出现两种编码：UTF8和GBK。因此，在探测时，只要区别这两种编码就可以了，UTF8比较通用，当文件中只有英文字符时，也默认它是UTF8，于是，只需要探测文件是否是GBK，如果不是，那么就是UTF8。

这里采用的探测方式是用正则匹配，当文件比较小时，是可以正常工作的，但是，当文件较大时，如果直接对整个文件进行正则匹配，由于正则匹配内部会进行递归，就会由于栈溢出直接导致程序出现段错误。因此，解决办法是，分别对每行进行正则匹配，如果探测出某行是GBK，那么整个文件就是GBK，如果每行都不是GBK，那么，整个文件就是UTF8。

* 文件读取时编码的转换

读取文件内容后，需要在前台进行显示，这时候，需要注意，前台展示时，一般用的是UTF8编码，因此，为了正确显示，需要在读取之后，将编码转换为UTF8。同样的，写入文件时，先探测文件编码，再将编码从UTF8转换为其它编码。

#### 2 crontab配置

* 前端crontab配置与检测

前端配置crontab时，将crontab抽象出几个通用的字段，例如：时间字段、执行脚本的目录、执行的脚本、输出的日志文件。通过分别配置这些字段，可以减少crontab写入失败的几率。其中，在格式校验方面，比较麻烦的就是crontab的时间字段的校验，可以到GitHub上查找这方面的组件。另一个问题是文件的执行权限，由于需要将crontab抽象为统一的结构，因此，这里不允许有`python test.py`的执行方式，因为这样处理起来就麻烦一些，这里采用统一的`./test.py`的方式，所以，test.py必须要有可执行权限。

* 后台crontab的写入

完成了crontab的配置，接下来就是要将crontab配置到机器。这里有两种方式可以将配置写入：

(1) 将crontab的默认编辑器修改为vi

(2) 修改/var/spool下的文件

方案(1)的优点是，可以用这种方式对crontab进行校验，同时，这样做，就需要对出错情况进行处理，处理不好容易导致进程挂死。方案(2)的优点是操作简单，直接对文件进行写入即可，缺点是，由于不同操作系统的crontab的文件的路径是不一样的，需要根据操作系统类型生成针对该系统的crontab路径名。

#### 3 文件权限

运维人员在包中配置启停脚本，或者在crontab中配置脚本执行时，经常会采用"./"的方式，这种方式要想正常工作，文件需要有执行权限。

但是，文件在从服务器发布到目标机器的过程中，中间会经过很多次拷贝过程，必须确保每次操作时，权限不丢失。但是，有时候无法保证中间过程权限不丢失，为了让这种执行方式正常工作，在发布到目标机器之前将包中的文件加上执行权限，文件的范围可以限定为二进制可执行程序、shell脚本、py脚本。

#### 4 数据库

* 误删数据库记录(binlog)

* 数据库的锁导致某条删除语句耗时过长

当数据库在执行SQL时，会锁住表，如果没有设置索引，会增大查找记录的耗时，并且会锁住整个表，当耗时很长时，可能会导致其它操作无法执行或者超时。这种情况，建议加上索引，有了索引，会减小查找记录的耗时，并且不需要锁住整个表。

* 数据库的记录超过最大长度

数据库中单条记录的长度是有限制的，即使某个字段设置为text，也不是任意长度都可以。因此，进行数据库操作时，需要有两个意识：

(1) 任何提交到数据库中的数据都需要限制长度，不是无限长的。

(2) 单行记录长度有限制，因此，在前端或者在程序中需要对提交的长度进行检测，限制插入和修改的记录长度。

* 将程序中的数据库的除了查询的语句输出到单独的日志

任何操作都需要有迹可循，数据库操作作为日志中比较特殊的一类操作，可以将日志中的非查询操作用于定位问题、恢复数据。因此，在写入日志时，可以将非查询操作的语句记录到单独的日志中。

#### 5 监控

* 可用性监控

不能依赖进程本身的状态，即使进程正常，也可能由于其它的原因导致操作失败。

* 与业务相关的监控

#### 6 "多"导致的问题

* 文件多导致生成MD5耗时过长

解决办法：设置忽略部分类型文件的规则

* 数据库记录多导致修改或者删除操作耗时过长

解决办法：增加索引，减少操作耗时

* 对机器上的文件的权限进行扫描，耗时很长导致任务异常而无法获取结果

解决办法：从文件类型、文件当前权限、文件名后缀等方面进行过滤，只过滤出需要操作的文件

* netstat -lntap

netstat用于查看当前的连接情况，该命令有几个很有用的选项：

```
-l 只显示监听的连接
-u 只显示UDP协议的连接
-t 只显示TCP协议的连接
-a 显示所有的连接
-n 显示数字
-p 显示连接的进程名
```

这里有一个需求：机器可能会有多个内网IP，需要获取可以登录的IP。

通常情况下，一台机器只有一个内网IP，而且内网IP和外网IP可以直接用接口进行区分，例如，eth1是内网IP，eth0是外网IP，此时，可以直接通过接口获取内网IP：`ifconfig eth1 | grep " inet " | awk '{print $2}' | awk -F':' '{print $2}'`。但是，采用这种方式极大地依赖机器的特性，在大部分机器上是可以的，但是在某些特殊的机器上不行。例如，inet addr与IP地址之间的分隔符可能不是冒号。总之，采用这样的方式依赖于命令的输出格式。

另一种方式是，通过`ip addr`加上awk就可以获取IP，思想跟前一种方法类似，同样的，也有类似的缺陷。

最后一种方式是，既然是获取可以登录的IP，那么，服务器一定是用该IP进行监听的，因此，可以查看服务器监听36000端口的IP:`netstat -ltn | grep 36000`，再使用正则表达式获取IP即可。

需要注意的是，这里使用-ltn选项，而是避免使用-a选项，当机器的连接较多，特别是对于web服务器而言，因此，这里使用-ltn选项即可。

#### 7 任何简单的操作都可能会失败

在远程执行脚本时，脚本中有一个拷贝动作：`cp -f`，正常情况下，该操作是不会失败的，而且，这里是以root用户执行，一定不会失败。但是，在脚本的执行结果日志中，发现有错误信息，信息提示：文件已存在。Google之后，是由于cp操作的判断操作和执行操作发生了竞争关系。一个解决办法是，在脚本中添加重试逻辑，但是，由于该操作出现的概率很小，如果这么个简单的拷贝动作都要重试，那么其它的拷贝代码全部要添加重试逻辑，这简直就是灾难。对于这样的情况，采取`发现`的方式就可以，也就是说：对于某些概率较小的或者暂时无法找到原因的问题，需要在代码里面添加检测逻辑，能够发现这些失败的情况。最怕的就是，出现了问题却无法发现，等到出了事故才去发现。